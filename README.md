---
license: apache-2.0
pipeline_tag: text-generation
---

**InternLM**

<div align="center">

<img src="https://github.com/InternLM/InternLM/assets/22529082/b9788105-8892-4398-8b47-b513a292378e" width="200"/>
  <div>&nbsp;</div>
  <div align="center">
    <b><font size="5">InternLM</font></b>
    <sup>
      <a href="https://internlm.intern-ai.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    <div>&nbsp;</div>
  </div>

[![evaluation](https://github.com/InternLM/InternLM/assets/22529082/f80a2a58-5ddf-471a-8da4-32ab65c8fd3b)](https://github.com/internLM/OpenCompass/)

[ğŸ¤”Reporting Issues](https://github.com/InternLM/InternLM/issues/new)

</div>


## Introduction

The Shanghai Artificial Intelligence Laboratory, in collaboration with SenseTime Technology, the Chinese University of Hong Kong, and Fudan University, has officially released the 20 billion parameter pretrained model, InternLM-20B. InternLM-20B was pre-trained on over **2.3T** Tokens containing high-quality English, Chinese, and code data. Additionally, the Chat version has undergone SFT and RLHF training, enabling it to better and more securely meet users' needs.

In terms of model structure, InternLM-20B opted for a deeper architecture, with a depth set at 60 layers. This surpasses the conventional 7B and 13B models that utilize 32 or 40 layers. When parameters are limited, increasing the number of layers can enhance the model's overall capability. Furthermore, compared to InternLM-7B, the pre-training data used for InternLM-20B underwent higher quality cleansing and was supplemented with data rich in knowledge and designed for reinforcing understanding and reasoning capabilities. As a result, it exhibits significant improvements in understanding, reasoning, mathematical, and programming abilitiesâ€”all of which test the technical proficiency of language models. Overall, InternLM-20B features the following characteristics:
- Outstanding overall performance
- Strong utility invocation capability
- Supports a 16k context length (Through infererence extrapolation)
- Better value alignment.

## Performance Evaluation
On the 5 capability dimensions proposed by OpenCompass, InternLM-20B has achieved excellent results (the bolded scores represent the best performances within the 13B-33B parameter range).

| Capability | Llama-13B | Llama2-13B | Baichuan2-13B | InternLM-20B | Llama-33B | Llama-65B | Llama2-70B |
|----------|-----------|------------|---------------|--------------|-----------|-----------|------------|
| Language     | 42.5      | 47         | 47.5          | **55**           | 44.6      | 47.1      | 51.6       |
| Knowledge     | 58.2      | 58.3       | 48.9          | 60.1         | **64**        | 66        | 67.7       |
| Understanding     | 45.5      | 50.9       | 58.1          | **67.3**         | 50.6      | 54.2      | 60.8       |
| Reasoning     | 42.7      | 43.6       | 44.2          | **54.9**         | 46.4      | 49.8      | 55         |
| Examination     | 37.3      | 45.2       | 51.8          | **62.5**         | 47.4      | 49.7      | 57.3       |
| Overall   | 43.8      | 47.3       | 49.4          | **59.2**         | 48.9      | 51.9      | 57.4       |

The table below compares the performance of mainstream open-source models on some influential and typical datasets.

|      | Benchmarks           | Llama-13B | Llama2-13B | Baichuan2-13B | InternLM-20B | Llama-33B | Llama-65B | Llama2-70B |
|------|------------------|-----------|------------|---------------|--------------|-----------|-----------|------------|
| Examination | MMLU             | 47.73     | 54.99      | 59.55         | **62.05**        | 58.73     | 63.71     | 69.75      |
|      | C-Eval (val)     | 31.83     | 41.4       | **59.01**         | 58.8         | 37.47     | 40.36     | 50.13      |
|      | AGI-Eval         | 22.03     | 30.93      | 37.37         | **44.58**        | 33.53     | 33.92     | 40.02      |
| Knowledge | BoolQ            | 78.75     | 82.42      | 67            | **87.46**        | 84.43     | 86.61     | 87.74      |
|      | TriviaQA         | 52.47     | 59.36      | 46.61         | 57.26        | **66.24**     | 69.79     | 70.71      |
|      | NaturalQuestions | 20.17     | 24.85      | 16.32         | 25.15        | **30.89**     | 33.41     | 34.16      |
| Understanding | CMRC             | 9.26      | 31.59      | 29.85         | **68.78**        | 14.17     | 34.73     | 43.74      |
|      | CSL              | 55        | 58.75      | 63.12         | **65.62**        | 57.5      | 59.38     | 60         |
|      | RACE (middle)    | 53.41     | 63.02      | 68.94         | **86.35**        | 64.55     | 72.35     | 81.55      |
|      | RACE (high)      | 47.63     | 58.86      | 67.18         | **83.28**        | 62.61     | 68.01     | 79.93      |
|      | XSum             | 20.37     | 23.37      | 25.23         | **35.54**        | 20.55     | 19.91     | 25.38      |
| Reasoning | WinoGrande       | 64.64     | 64.01      | 67.32         | **69.38**        | 66.85     | 69.38     | 69.77      |
|      | BBH              | 37.93     | 45.62      | 48.98         | **52.51**        | 49.98     | 58.38     | 64.91      |
|      | GSM8K            | 20.32     | 29.57      | **52.62**         | **52.62**        | 42.3      | 54.44     | 63.31      |
|      | PIQA             | 79.71     | 79.76      | 78.07         | 80.25        | **81.34**     | 82.15     | 82.54      |
| Programming | HumanEval        | 14.02     | 18.9       | 17.07         | **25.61**        | 17.68     | 18.9      | 26.22      |
|      | MBPP             | 20.6      | 26.8       | 30.8          | **35.6**         | 28.4      | 33.6      | 39.6       |

Overall, InternLM-20B comprehensively outperforms open-source models in the 13B parameter range in terms of overall capabilities, and on inference evaluation sets, it approaches or even surpasses the performance of Llama-65B.

## Import from Transformers
To load the InternLM 7B Chat model using Transformers, use the following code:
```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> tokenizer = AutoTokenizer.from_pretrained("internlm/internlm-20b", trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("internlm/internlm-20b", trust_remote_code=True).cuda()
>>> model = model.eval()
>>> inputs = tokenizer(["Coming to the beautiful nature, we found"], return_tensors="pt")
>>> for k,v in inputs.items():
        inputs[k] = v.cuda()
>>> gen_kwargs = {"max_length": 128, "top_p": 0.8, "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.05}
>>> output = model.generate(**inputs, **gen_kwargs)
>>> output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)
>>> print(output)
Coming to the beautiful nature, we found not only various mountains, rivers, trees, and flowers but also many birds and beasts. Birds are the ones we are most familiar with; some are soaring in the sky, some are hopping on the ground, while others perch on trees...
```

**Limitations:** Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.


## Open Source License

The code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow **free** commercial usage. To apply for a commercial license, please fill in the [application form (English)](https://wj.qq.com/s2/12727483/5dba/)/[ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰](https://wj.qq.com/s2/12725412/f7c1/). For other questions or collaborations, please contact <internlm@pjlab.org.cn>.


## ç®€ä»‹
ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ä¸å•†æ±¤ç§‘æŠ€è”åˆé¦™æ¸¯ä¸­æ–‡å¤§å­¦å’Œå¤æ—¦å¤§å­¦æ­£å¼æ¨å‡ºä¹¦ç”ŸÂ·æµ¦è¯­200äº¿å‚æ•°æ¨¡å‹ç‰ˆæœ¬ InternLM-20B ï¼ŒInternLM-20B åœ¨è¶…è¿‡ **2.3T** Tokens åŒ…å«é«˜è´¨é‡è‹±æ–‡ã€ä¸­æ–‡å’Œä»£ç çš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶ä¸­ Chat ç‰ˆæœ¬è¿˜ç»è¿‡äº† SFT å’Œ RLHF è®­ç»ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½ã€æ›´å®‰å…¨åœ°æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ã€‚  

InternLM 20B åœ¨æ¨¡å‹ç»“æ„ä¸Šé€‰æ‹©äº†æ·±ç»“æ„ï¼Œå±‚æ•°è®¾å®šä¸º60å±‚ï¼Œè¶…è¿‡å¸¸è§„7Bå’Œ13Bæ¨¡å‹æ‰€ä½¿ç”¨çš„32å±‚æˆ–è€…40å±‚ã€‚åœ¨å‚æ•°å—é™çš„æƒ…å†µä¸‹ï¼Œæé«˜å±‚æ•°æœ‰åˆ©äºæé«˜æ¨¡å‹çš„ç»¼åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºInternLM-7Bï¼ŒInternLM-20Bä½¿ç”¨çš„é¢„è®­ç»ƒæ•°æ®ç»è¿‡äº†æ›´é«˜è´¨é‡çš„æ¸…æ´—ï¼Œå¹¶è¡¥å……äº†é«˜çŸ¥è¯†å¯†åº¦å’Œç”¨äºå¼ºåŒ–ç†è§£ä¸æ¨ç†èƒ½åŠ›çš„è®­ç»ƒæ•°æ®ã€‚å› æ­¤ï¼Œå®ƒåœ¨ç†è§£èƒ½åŠ›ã€æ¨ç†èƒ½åŠ›ã€æ•°å­¦èƒ½åŠ›ã€ç¼–ç¨‹èƒ½åŠ›ç­‰è€ƒéªŒè¯­è¨€æ¨¡å‹æŠ€æœ¯æ°´å¹³çš„æ–¹é¢éƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ€»ä½“è€Œè¨€ï¼ŒInternLM-20Bå…·æœ‰ä»¥ä¸‹çš„ç‰¹ç‚¹ï¼š 
- ä¼˜å¼‚çš„ç»¼åˆæ€§èƒ½
- å¾ˆå¼ºçš„å·¥å…·è°ƒç”¨åŠŸèƒ½
- æ”¯æŒ16kè¯­å¢ƒé•¿åº¦ï¼ˆé€šè¿‡æ¨ç†æ—¶å¤–æ¨ï¼‰
- æ›´å¥½çš„ä»·å€¼å¯¹é½

## æ€§èƒ½è¯„æµ‹
åœ¨OpenCompassæå‡ºçš„5ä¸ªèƒ½åŠ›ç»´åº¦ä¸Šï¼ŒInternLM-20Béƒ½å–å¾—å¾ˆå¥½çš„æ•ˆæœï¼ˆç²—ä½“ä¸º13B-33Bè¿™ä¸ªé‡çº§èŒƒå›´å†…ï¼Œå„é¡¹æœ€ä½³æˆç»©ï¼‰

| èƒ½åŠ›ç»´åº¦ | Llama-13B | Llama2-13B | Baichuan2-13B | InternLM-20B | Llama-33B | Llama-65B | Llama2-70B |
|----------|-----------|------------|---------------|--------------|-----------|-----------|------------|
| è¯­è¨€     | 42.5      | 47         | 47.5          | **55**           | 44.6      | 47.1      | 51.6       |
| çŸ¥è¯†     | 58.2      | 58.3       | 48.9          | 60.1         | **64**        | 66        | 67.7       |
| ç†è§£     | 45.5      | 50.9       | 58.1          | **67.3**         | 50.6      | 54.2      | 60.8       |
| æ¨ç†     | 42.7      | 43.6       | 44.2          | **54.9**         | 46.4      | 49.8      | 55         |
| å­¦ç§‘     | 37.3      | 45.2       | 51.8          | **62.5**         | 47.4      | 49.7      | 57.3       |
| æ€»å¹³å‡   | 43.8      | 47.3       | 49.4          | **59.2**         | 48.9      | 51.9      | 57.4       |

ä¸‹è¡¨å±•ç¤ºäº†åœ¨å¤šä¸ªç»å…¸æ•°æ®é›†ä¸Š InternLM 20B ä¸å„ä¸ªä¸»æµå¼€æºæ¨¡å‹çš„è¡¨ç°

|      | è¯„æµ‹é›†           | Llama-13B | Llama2-13B | Baichuan2-13B | InternLM-20B | Llama-33B | Llama-65B | Llama2-70B |
|------|------------------|-----------|------------|---------------|--------------|-----------|-----------|------------|
| å­¦ç§‘ | MMLU             | 47.73     | 54.99      | 59.55         | **62.05**        | 58.73     | 63.71     | 69.75      |
|      | C-Eval (val)     | 31.83     | 41.4       | **59.01**         | 58.8         | 37.47     | 40.36     | 50.13      |
|      | AGI-Eval         | 22.03     | 30.93      | 37.37         | **44.58**        | 33.53     | 33.92     | 40.02      |
| çŸ¥è¯† | BoolQ            | 78.75     | 82.42      | 67            | **87.46**        | 84.43     | 86.61     | 87.74      |
|      | TriviaQA         | 52.47     | 59.36      | 46.61         | 57.26        | **66.24**     | 69.79     | 70.71      |
|      | NaturalQuestions | 20.17     | 24.85      | 16.32         | 25.15        | **30.89**     | 33.41     | 34.16      |
| ç†è§£ | CMRC             | 9.26      | 31.59      | 29.85         | **68.78**        | 14.17     | 34.73     | 43.74      |
|      | CSL              | 55        | 58.75      | 63.12         | **65.62**        | 57.5      | 59.38     | 60         |
|      | RACE (middle)    | 53.41     | 63.02      | 68.94         | **86.35**        | 64.55     | 72.35     | 81.55      |
|      | RACE (high)      | 47.63     | 58.86      | 67.18         | **83.28**        | 62.61     | 68.01     | 79.93      |
|      | XSum             | 20.37     | 23.37      | 25.23         | **35.54**        | 20.55     | 19.91     | 25.38      |
| æ¨ç† | WinoGrande       | 64.64     | 64.01      | 67.32         | **69.38**        | 66.85     | 69.38     | 69.77      |
|      | BBH              | 37.93     | 45.62      | 48.98         | **52.51**        | 49.98     | 58.38     | 64.91      |
|      | GSM8K            | 20.32     | 29.57      | **52.62**         | **52.62**        | 42.3      | 54.44     | 63.31      |
|      | PIQA             | 79.71     | 79.76      | 78.07         | 80.25        | **81.34**     | 82.15     | 82.54      |
| ç¼–ç¨‹ | HumanEval        | 14.02     | 18.9       | 17.07         | **25.61**        | 17.68     | 18.9      | 26.22      |
|      | MBPP             | 20.6      | 26.8       | 30.8          | **35.6**         | 28.4      | 33.6      | 39.6       |

æ€»ä½“è€Œè¨€ï¼ŒInternLM-20B åœ¨ç»¼åˆèƒ½åŠ›ä¸Šå…¨é¢é¢†å…ˆäº13Bé‡çº§çš„å¼€æºæ¨¡å‹ï¼ŒåŒæ—¶åœ¨æ¨ç†è¯„æµ‹é›†ä¸Šèƒ½å¤Ÿæ¥è¿‘ç”šè‡³è¶…è¶ŠLlama-65Bçš„æ€§èƒ½ã€‚

## é€šè¿‡ Transformers åŠ è½½
é€šè¿‡ä»¥ä¸‹çš„ä»£ç åŠ è½½ InternLM 20B æ¨¡å‹
```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> tokenizer = AutoTokenizer.from_pretrained("internlm/internlm-20b", trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("internlm/internlm-20b", trust_remote_code=True).cuda()
>>> model = model.eval()
>>> inputs = tokenizer(["æ¥åˆ°ç¾ä¸½çš„å¤§è‡ªç„¶ï¼Œæˆ‘ä»¬å‘ç°"], return_tensors="pt")
>>> for k,v in inputs.items():
        inputs[k] = v.cuda()
>>> gen_kwargs = {"max_length": 128, "top_p": 0.8, "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.05}
>>> output = model.generate(**inputs, **gen_kwargs)
>>> output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)
>>> print(output)
æ¥åˆ°ç¾ä¸½çš„å¤§è‡ªç„¶ï¼Œæˆ‘ä»¬å‘ç°ï¼Œè¿™é‡Œä¸ä»…æœ‰å¤§å¤§å°å°çš„å±±å·æ²³æµå’Œæ ‘æœ¨èŠ±è‰ï¼Œè€Œä¸”è¿˜æœ‰å¾ˆå¤šé£é¸Ÿèµ°å…½ã€‚æˆ‘ä»¬æœ€ç†Ÿæ‚‰çš„å°±æ˜¯é¸Ÿç±»äº†ï¼Œå®ƒä»¬æœ‰çš„åœ¨å¤©ä¸Šé£ç¿”ï¼Œæœ‰çš„åœ¨åœ°ä¸Šè·³è·ƒï¼Œè¿˜æœ‰çš„åœ¨æ ‘ä¸Šæ –æ¯â€¦â€¦
```

**å±€é™æ€§ï¼š** å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éå¸¸æ³¨é‡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºæ¨¡å‹å¤§å°ä»¥åŠæ¦‚ç‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›å¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åæœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚

## å¼€æºè®¸å¯è¯

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸åˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚
